{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base packages\n",
    "import math, os, scipy, h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv \n",
    "from tldextract import extract\n",
    "\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras import preprocessing\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_datasets\n",
    "#!pip install tensorflow-text\n",
    "#!pip install tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()\n",
    "\n",
    "EMBED_SIZE=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "usrname = os.getenv('MYSQL_USER')\n",
    "passwd = os.getenv('MYSQL_PASSWORD')\n",
    "dbname = os.getenv('MYSQL_DB')\n",
    "\n",
    "connectstring = \"mysql://\"+usrname+ \":\"+passwd+\"@localhost/\"+dbname+\"?charset=utf8\"\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "dbengine = create_engine(connectstring,encoding=\"utf8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations 17522\n",
      "     count  mean  std  min  25%  50%  75%  max\n",
      "y                                             \n",
      "0  15963.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "1   1448.0   1.0  0.0  1.0  1.0  1.0  1.0  1.0\n",
      "2    111.0   2.0  0.0  2.0  2.0  2.0  2.0  2.0\n"
     ]
    }
   ],
   "source": [
    "#Load #phishnet database\n",
    "#use the DB/ This is only for you.\n",
    "\n",
    "def load_url_data():\n",
    "  \n",
    "    \n",
    "    myQuery = '''select tmp1.url,if(click.clicked_dt is null, tmp1.y, 2) as y  from \n",
    "                (SELECT url, 1 as y FROM ternary_fund.redditdata\n",
    "                 union select url, 1 as y from RecNet.reddit_urls\n",
    "                 union select url, 0 as y from RecNet.phishing_data) as tmp1\n",
    "\n",
    "                left join RecNet.click_data as click on\n",
    "                MD5(tmp1.url) = MD5(click.url)\n",
    "                \n",
    "                order by rand();\n",
    "                '''\n",
    "    df = pd.read_sql_query(myQuery, dbengine)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Pandas dataframe\n",
    "df = load_url_data()\n",
    "print(\"Number of observations \" +str(len(df.index)))\n",
    "grouped_df=df.groupby(['y'])['y']\n",
    "\n",
    "print(grouped_df.describe())\n",
    "    \n",
    "df = df.drop_duplicates()\n",
    "y_data = df['y'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#URL tokenizer\n",
    "#INPUT  : A utf-8 encoded URL String\n",
    "#OUTOUT : A dict of tokens\n",
    "\n",
    "def getTokens(urlinput):\n",
    "    tokensBySlash=str(urlinput.encode('utf-8')).split('/')\n",
    "    allTokens = []\n",
    "    for i in tokensBySlash:\n",
    "        tokens = str(i).split('-')\n",
    "        tokensByDot=[]\n",
    "        for j in range(0,len(tokens)):\n",
    "            tempTokens = str(tokens[j]).split('.')\n",
    "            tokensByDot = tokensByDot + tempTokens\n",
    "        allTokens=allTokens+tokens+tokensByDot\n",
    "    allTokens = list(set(allTokens))\n",
    "    \n",
    "    if 'com' in allTokens:\n",
    "        allTokens.remove('com')\n",
    "        \n",
    "    return allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getTokens_into_characters(urlinput):\n",
    "    tokensBySlash=' '.join(str(urlinput.encode('utf-8')).strip(\"'\").strip(\"b'\")).split('/')\n",
    "    #print(tokensBySlash)\n",
    "    allTokens = []\n",
    "    for i in tokensBySlash:\n",
    "        tokens = str(i).split('-')\n",
    "        #print(\"*** splitting by dash ***\")\n",
    "        #print(tokens)\n",
    "        tokensByDot=[]\n",
    "        for j in range(0,len(tokens)):\n",
    "            #print(str(tokens[j]).split('.'))\n",
    "            tokensByDot += str(tokens[j]).split('.')\n",
    "            #print(\"*** splitting by dot ***\")\n",
    "            #print(tokensByDot)\n",
    "        allTokens+=tokensByDot\n",
    "    allTokens = ' '.join(list(allTokens)).split() \n",
    "    \n",
    "    #print(allTokens)\n",
    "    #if 'com' in allTokens:\n",
    "    #    allTokens.remove('com')\n",
    "       \n",
    "    return allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getTokens_into_words(urlinput):\n",
    "    tokensBySlash=str(urlinput.encode('utf-8')).strip(\"'\").strip(\"b'\").split('/')\n",
    "    #print(tokensBySlash)\n",
    "    allTokens = []\n",
    "    for i in tokensBySlash:\n",
    "        tokens = str(i).split('-')\n",
    "        #print(\"*** splitting by dash ***\")\n",
    "        #print(tokens)\n",
    "        tokensByDot=[]\n",
    "        for j in range(0,len(tokens)):\n",
    "            #print(str(tokens[j]).split('.'))\n",
    "            tokensByDot += str(tokens[j]).split('.')\n",
    "            #print(\"*** splitting by dot ***\")\n",
    "            #print(tokensByDot)\n",
    "        allTokens+=tokensByDot\n",
    "    allTokens = ' '.join(list(allTokens)).split() \n",
    "    \n",
    "    #print(allTokens)\n",
    "    #if 'com' in allTokens:\n",
    "    #    allTokens.remove('com')\n",
    "       \n",
    "    return allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getURLInfo(url):\n",
    "    #some idea to extract only once\n",
    "    val = extract(url)\n",
    "    a = val.domain\n",
    "    b = val.suffix\n",
    "    \n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_url_word = df.apply(lambda row:getTokens_into_words(row['url']), axis=1)\n",
    "df.insert(len(df.columns), 'tokenized_url_word',tokenized_url_word )\n",
    "\n",
    "tokenized_url_char= df.apply(lambda row:getTokens_into_characters(row['url']), axis=1)\n",
    "df.insert(len(df.columns), 'tokenized_url_char',tokenized_url_char )\n",
    "\n",
    "tld= df.apply(lambda row:extract(row['url']).suffix, axis=1)\n",
    "df.insert(len(df.columns), 'tld',tld )\n",
    "\n",
    "domain = df.apply(lambda row:extract(row['url']).domain, axis=1)\n",
    "df.insert(len(df.columns), 'domain',domain )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for testing the tokenization effort  ; commented out since it works\n",
    "#df[['tokenized_url_word','url','domain','tld']].to_csv('tokenized-result.csv')   \n",
    "#what does that really measure?\n",
    "#val= df.tokenized_url_word.map(len).max()\n",
    "#print(\"Longest vector with words \" + str(val))\n",
    "\n",
    "#focus embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the URL from text sequences to integers per vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17515, 20)\n",
      "*** sample check ***\n",
      "['http:', '92', '119', '115', '94', 'xmlrpc', 'php']\n",
      "[   3 1136 1363 2443  843 8674    7    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "['http:', 'rangelferreira', 'adv', 'br']\n",
      "[   3 8675 2444   35    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tf_keras_tokenizer = Tokenizer()\n",
    "tf_keras_tokenizer.fit_on_texts(df['tokenized_url_word'])\n",
    "tf_keras_encoded = tf_keras_tokenizer.texts_to_sequences(df['tokenized_url_word'])\n",
    "tf_keras_encoded = preprocessing.sequence.pad_sequences(tf_keras_encoded, padding=\"post\", maxlen=EMBED_SIZE) \n",
    "print(tf_keras_encoded.shape)\n",
    "\n",
    "print(\"*** sample check ***\")\n",
    "print(df['tokenized_url_word'][1])\n",
    "print(tf_keras_encoded[1] )\n",
    "\n",
    "print(df['tokenized_url_word'][2])\n",
    "print(tf_keras_encoded[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'betasus6'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_keras_tokenizer.index_word[5000]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the Q-learning approach\n",
    "#We have now a tokenized representation of the url\n",
    "#Next step token to integer\n",
    "#then integer to sense vector (the embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27348\n"
     ]
    }
   ],
   "source": [
    "#print(max(encoded_urls))\n",
    "print(max(tf_keras_tokenizer.index_word ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layers\n",
    "An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters \n",
    "(weights learned by the model during training, in the same way a model learns weights for a dense layer). \n",
    "It is common to see word embeddings that are 8-dimensional (for small datasets),\n",
    "up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained\n",
    "relationships between words, but takes more data to learn.\n",
    "\n",
    "The Embedding layer can be understood as a lookup table that maps from integer indices \n",
    "(which stand for specific words) to dense vectors (their embeddings).\n",
    "The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27349\n",
      "(17515, 20, 20)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[ 0.0235973   0.00458556  0.03265947  0.02491995  0.01176801  0.03566331\n",
      "   0.02945307  0.04119417  0.01167879 -0.00541974 -0.01030164  0.04945549\n",
      "   0.03076721 -0.01643503  0.04844477 -0.04776831 -0.04961973 -0.01994123\n",
      "  -0.04586731  0.03668361]\n",
      " [-0.04161888  0.00960376  0.04958213  0.01719692  0.04383865 -0.03821521\n",
      "   0.00884803  0.01316024  0.00709238 -0.04462513  0.0085342  -0.02788415\n",
      "  -0.0020823   0.02606661 -0.01360477  0.02249715  0.03385003  0.04130704\n",
      "   0.04059443  0.01198201]\n",
      " [ 0.04853312  0.04253754  0.04314449 -0.01307096  0.03372278 -0.04000518\n",
      "   0.0305846   0.03284589  0.01849871 -0.04812187  0.00838912  0.0123531\n",
      "  -0.02486642 -0.01130117  0.00907809  0.01639544 -0.00414705  0.00522097\n",
      "   0.01436586 -0.0228098 ]\n",
      " [-0.01335535 -0.04597292 -0.01674479  0.00064097  0.02425278 -0.04278617\n",
      "   0.0218132  -0.03670419  0.04429739  0.02804651 -0.03418125 -0.01292949\n",
      "   0.04862865 -0.01371051 -0.01027545  0.00763296 -0.01983355  0.04678133\n",
      "   0.0082515  -0.00475989]\n",
      " [ 0.04923576 -0.04505111  0.0465328  -0.04758769  0.03036532  0.014601\n",
      "   0.01028761  0.00723843  0.02988262 -0.01678212 -0.02842723  0.00358189\n",
      "   0.04570217 -0.04577552 -0.03543638 -0.01443153  0.00259236 -0.01330899\n",
      "  -0.01519904  0.02927849]\n",
      " [ 0.0458899   0.02199228  0.00955454 -0.0132854  -0.00976583 -0.02298516\n",
      "   0.00490266 -0.00387298 -0.00815262  0.03492217  0.00643562  0.02166129\n",
      "   0.0241144   0.02915427 -0.04195314 -0.02417697  0.04761139  0.01686415\n",
      "  -0.02312478  0.01590017]\n",
      " [ 0.03545139  0.00839362 -0.0336237  -0.02712834  0.04916087 -0.01986106\n",
      "  -0.03923134 -0.00864409 -0.02252003  0.01644809 -0.04454048  0.01744897\n",
      "  -0.00524179 -0.04853072  0.04741938  0.04395762  0.03224964  0.00638088\n",
      "  -0.02247989  0.03153117]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]\n",
      " [ 0.02739627 -0.04464277 -0.03755866 -0.01425384  0.04321769  0.01167407\n",
      "  -0.03654851 -0.00825135  0.03693013  0.01134378 -0.03486443 -0.01648525\n",
      "  -0.02365035 -0.008319   -0.0251564  -0.00021826  0.04674745 -0.0446144\n",
      "   0.04911229 -0.01805192]], shape=(20, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_LEN = max(tf_keras_tokenizer.index_word ) +1\n",
    "print(VOCAB_LEN)\n",
    "\n",
    "embedding_layer = layers.Embedding(VOCAB_LEN, EMBED_SIZE)\n",
    "\n",
    "result = embedding_layer(tf_keras_encoded)\n",
    "#result.numpy()\n",
    "\n",
    "#Voila, now I have a list of tokenized, vocabbed, and embedded words. \n",
    "#Happiness arises\n",
    "\n",
    "print(result.shape)\n",
    "print(type(result))\n",
    "print(result[5000]) #should be a EMBED_SIZE x EMBED_SIZE expression of 'revolut'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on).\n",
    "\n",
    "If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes (32, 10) (batch of 32 sequences of length 10) or (64, 15) (batch of 64 sequences of length 15).\n",
    "\n",
    "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a (2, 3) input batch and the output is (2, 3, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17515, 20, 20)\n",
      "(17515, 20, 20, 1)\n",
      "(17515,)\n",
      "(17515, 1)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]], shape=(17515, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#Split the generates tf_keras_encoded dataset (result) into training and testing\n",
    "#DATASET_SIZE = result.shape[0]\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((result, y_data))\n",
    "dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "result2 = tf.reshape(result, (-1, EMBED_SIZE, EMBED_SIZE,1))\n",
    "y_data2 = tf.reshape(y_data, (-1, 1))\n",
    "\n",
    "print(result.shape)\n",
    "print(result2.shape)\n",
    "print(y_data.shape)\n",
    "print(y_data2.shape)\n",
    "\n",
    "print(type(result))\n",
    "print(type(result2))\n",
    "\n",
    "print(y_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14515, 20, 20, 1)\n",
      "(14515, 1)\n",
      "(3000, 20, 20, 1)\n",
      "(3000, 1)\n"
     ]
    }
   ],
   "source": [
    "#hardcode the validation sample at 3000\n",
    "#not good practice, but hey, here we are\n",
    "x_val = result2[-3000:]\n",
    "y_val = y_data2[-3000:]\n",
    "x_train = result2[:-3000]\n",
    "y_train = y_data2[:-3000]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#256 h-length convolutional filters 3,4,5,6 - for WORD level\n",
    "wordmodel = models.Sequential()\n",
    "wordmodel.add(layers.Conv2D(filters=256,kernel_size=(3, 3), strides= (1, 1), padding='same', activation='relu', input_shape=(EMBED_SIZE, EMBED_SIZE, 1)))\n",
    "wordmodel.add(layers.MaxPooling2D((2, 2)))\n",
    "wordmodel.add(layers.Dense(512, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmodel.add(layers.Flatten())\n",
    "wordmodel.add(layers.Dense(512, activation='relu'))\n",
    "wordmodel.add(layers.Dense(256, activation='relu'))\n",
    "wordmodel.add(layers.Dense(128, activation='relu'))\n",
    "wordmodel.add(layers.Dense(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmodel.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),metrics=['accuracy'])\n",
    "\n",
    "#wordmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 20, 20, 256)       2560      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10, 10, 512)       131584    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               26214912  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 26,513,667\n",
      "Trainable params: 26,513,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wordmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "304/409 [=====================>........] - ETA: 26s - loss: 1.4225 - accuracy: 0.9093"
     ]
    }
   ],
   "source": [
    "#Reminder change the epoch if needed it was 10\n",
    "#Why you need the epoch anyways...\n",
    "\n",
    "history = wordmodel.fit(x_train, y_train, epochs=2, validation_split=0.1, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = wordmodel.evaluate(x_val,y_val, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = wordmodel.predict(x_val[:3])\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling and fitting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmodel.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['loss'], label = 'loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = wordmodel.evaluate(result2,  y_data2, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
