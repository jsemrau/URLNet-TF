{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base packages\n",
    "import math, os, scipy, h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv \n",
    "from tldextract import extract\n",
    "\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras import preprocessing\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_datasets\n",
    "#!pip install tensorflow-text\n",
    "#!pip install tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()\n",
    "\n",
    "EMBED_SIZE=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "usrname = os.getenv('MYSQL_USER')\n",
    "passwd = os.getenv('MYSQL_PASSWORD')\n",
    "dbname = os.getenv('MYSQL_DB')\n",
    "\n",
    "connectstring = \"mysql://\"+usrname+ \":\"+passwd+\"@localhost/\"+dbname+\"?charset=utf8\"\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "dbengine = create_engine(connectstring,encoding=\"utf8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations 17865\n",
      "     count  mean  std  min  25%  50%  75%  max\n",
      "y                                             \n",
      "0  15963.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "1   1761.0   1.0  0.0  1.0  1.0  1.0  1.0  1.0\n",
      "2    141.0   2.0  0.0  2.0  2.0  2.0  2.0  2.0\n"
     ]
    }
   ],
   "source": [
    "#Load #phishnet database\n",
    "#use the DB/ This is only for you.\n",
    "\n",
    "def load_url_data():\n",
    "  \n",
    "    \n",
    "    myQuery = '''select tmp1.url,if(click.clicked_dt is null, tmp1.y, 2) as y  from \n",
    "                (SELECT url, 1 as y FROM ternary_fund.redditdata\n",
    "                 union select url, 1 as y from RecNet.reddit_urls where domain != \"i.redd.it\"\n",
    "                 union select url, 0 as y from RecNet.phishing_data) as tmp1\n",
    "\n",
    "                left join RecNet.click_data as click on\n",
    "                MD5(tmp1.url) = MD5(click.url)\n",
    "                \n",
    "                order by rand();\n",
    "                '''\n",
    "    df = pd.read_sql_query(myQuery, dbengine)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Pandas dataframe\n",
    "df = load_url_data()\n",
    "print(\"Number of observations \" +str(len(df.index)))\n",
    "grouped_df=df.groupby(['y'])['y']\n",
    "\n",
    "print(grouped_df.describe())\n",
    "    \n",
    "df = df.drop_duplicates()\n",
    "y_data = df['y'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#URL tokenizer\n",
    "#INPUT  : A utf-8 encoded URL String\n",
    "#OUTOUT : A dict of tokens\n",
    "\n",
    "def getTokens(urlinput):\n",
    "    tokensBySlash=str(urlinput.encode('utf-8')).split('/')\n",
    "    allTokens = []\n",
    "    for i in tokensBySlash:\n",
    "        tokens = str(i).split('-')\n",
    "        tokensByDot=[]\n",
    "        for j in range(0,len(tokens)):\n",
    "            tempTokens = str(tokens[j]).split('.')\n",
    "            tokensByDot = tokensByDot + tempTokens\n",
    "        allTokens=allTokens+tokens+tokensByDot\n",
    "    allTokens = list(set(allTokens))\n",
    "    \n",
    "    if 'com' in allTokens:\n",
    "        allTokens.remove('com')\n",
    "        \n",
    "    return allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getTokens_into_characters(urlinput):\n",
    "    tokensBySlash=' '.join(str(urlinput.encode('utf-8')).strip(\"'\").strip(\"b'\")).split('/')\n",
    "    #print(tokensBySlash)\n",
    "    allTokens = []\n",
    "    for i in tokensBySlash:\n",
    "        tokens = str(i).split('-')\n",
    "        #print(\"*** splitting by dash ***\")\n",
    "        #print(tokens)\n",
    "        tokensByDot=[]\n",
    "        for j in range(0,len(tokens)):\n",
    "            #print(str(tokens[j]).split('.'))\n",
    "            tokensByDot += str(tokens[j]).split('.')\n",
    "            #print(\"*** splitting by dot ***\")\n",
    "            #print(tokensByDot)\n",
    "        allTokens+=tokensByDot\n",
    "    allTokens = ' '.join(list(allTokens)).split() \n",
    "    \n",
    "    #print(allTokens)\n",
    "    #if 'com' in allTokens:\n",
    "    #    allTokens.remove('com')\n",
    "       \n",
    "    return allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getTokens_into_words(urlinput):\n",
    "    tokensBySlash=str(urlinput.encode('utf-8')).strip(\"'\").strip(\"b'\").split('/')\n",
    "    #print(tokensBySlash)\n",
    "    allTokens = []\n",
    "    for i in tokensBySlash:\n",
    "        tokens = str(i).split('-')\n",
    "        #print(\"*** splitting by dash ***\")\n",
    "        #print(tokens)\n",
    "        tokensByDot=[]\n",
    "        for j in range(0,len(tokens)):\n",
    "            #print(str(tokens[j]).split('.'))\n",
    "            tokensByDot += str(tokens[j]).split('.')\n",
    "            #print(\"*** splitting by dot ***\")\n",
    "            #print(tokensByDot)\n",
    "        allTokens+=tokensByDot\n",
    "    allTokens = ' '.join(list(allTokens)).split() \n",
    "    \n",
    "    #print(allTokens)\n",
    "    #if 'com' in allTokens:\n",
    "    #    allTokens.remove('com')\n",
    "       \n",
    "    return allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getURLInfo(url):\n",
    "    #some idea to extract only once\n",
    "    val = extract(url)\n",
    "    a = val.domain\n",
    "    b = val.suffix\n",
    "    \n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_url_word = df.apply(lambda row:getTokens_into_words(row['url']), axis=1)\n",
    "df.insert(len(df.columns), 'tokenized_url_word',tokenized_url_word )\n",
    "\n",
    "tokenized_url_char= df.apply(lambda row:getTokens_into_characters(row['url']), axis=1)\n",
    "df.insert(len(df.columns), 'tokenized_url_char',tokenized_url_char )\n",
    "\n",
    "tld= df.apply(lambda row:extract(row['url']).suffix, axis=1)\n",
    "df.insert(len(df.columns), 'tld',tld )\n",
    "\n",
    "domain = df.apply(lambda row:extract(row['url']).domain, axis=1)\n",
    "df.insert(len(df.columns), 'domain',domain )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for testing the tokenization effort  ; commented out since it works\n",
    "#df[['tokenized_url_word','url','domain','tld']].to_csv('tokenized-result.csv')   \n",
    "#what does that really measure?\n",
    "#val= df.tokenized_url_word.map(len).max()\n",
    "#print(\"Longest vector with words \" + str(val))\n",
    "\n",
    "#focus embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the URL from text sequences to integers per vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17796, 20)\n",
      "*** sample check ***\n",
      "http://granhongo.com/wp-includes/Login/customer_center/customer-IDPP00C824/myaccount/identity/?cmd=_session=US&amp;;0595c7a41dc9fe6ff5c81eb916bf2461&amp;;dispatch=3ca119e0036f5dd52757270e38d3b0322f965418\n",
      "['http:', 'granhongo', 'com', 'wp', 'includes', 'Login', 'customer_center', 'customer', 'IDPP00C824', 'myaccount', 'identity', '?cmd=_session=US&amp;;0595c7a41dc9fe6ff5c81eb916bf2461&amp;;dispatch=3ca119e0036f5dd52757270e38d3b0322f965418']\n",
      "[   3  766    1    5   19    6   62   61  767   44  165 9384    0    0\n",
      "    0    0    0    0    0    0]\n",
      "https://paypal-paiement.nhx.fr/myaccount/signin/?country.x=US&locale.x=en_US\n",
      "['https:', 'paypal', 'paiement', 'nhx', 'fr', 'myaccount', 'signin', '?country', 'x=US&locale', 'x=en_US']\n",
      "[   2   29 2237 2238   65   44   51  172  438  285    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tf_keras_tokenizer = Tokenizer()\n",
    "tf_keras_tokenizer.fit_on_texts(df['tokenized_url_word'])\n",
    "tf_keras_encoded = tf_keras_tokenizer.texts_to_sequences(df['tokenized_url_word'])\n",
    "tf_keras_encoded = preprocessing.sequence.pad_sequences(tf_keras_encoded, padding=\"post\", maxlen=EMBED_SIZE) \n",
    "print(tf_keras_encoded.shape)\n",
    "\n",
    "print(\"*** sample check ***\")\n",
    "print(df['url'][100])\n",
    "print(df['tokenized_url_word'][100])\n",
    "print(tf_keras_encoded[100] )\n",
    "\n",
    "print(df['url'][200])\n",
    "print(df['tokenized_url_word'][200])\n",
    "print(tf_keras_encoded[200] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resbetsgiris'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_keras_tokenizer.index_word[5000]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the Q-learning approach\n",
    "#We have now a tokenized representation of the url\n",
    "#Next step token to integer\n",
    "#then integer to sense vector (the embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28892\n"
     ]
    }
   ],
   "source": [
    "#print(max(encoded_urls))\n",
    "print(max(tf_keras_tokenizer.index_word ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layers\n",
    "An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters \n",
    "(weights learned by the model during training, in the same way a model learns weights for a dense layer). \n",
    "It is common to see word embeddings that are 8-dimensional (for small datasets),\n",
    "up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained\n",
    "relationships between words, but takes more data to learn.\n",
    "\n",
    "The Embedding layer can be understood as a lookup table that maps from integer indices \n",
    "(which stand for specific words) to dense vectors (their embeddings).\n",
    "The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28893\n",
      "(17796, 20, 20)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[-0.04289266 -0.00011485 -0.03722057 -0.00737196  0.01851607 -0.00184865\n",
      "   0.02960584 -0.01173165  0.03141943  0.03511794 -0.01993811 -0.00697229\n",
      "  -0.03550835  0.00446681 -0.03122972  0.02155412 -0.03519147  0.02166437\n",
      "   0.03360565 -0.01395231]\n",
      " [ 0.02416639  0.0296557   0.04601595 -0.01289277 -0.00420254  0.02007831\n",
      "  -0.01219572  0.02458297 -0.02216781  0.02697629  0.03359176  0.01349349\n",
      "   0.02408506 -0.02799667 -0.01883093  0.04949525  0.04161314  0.03738426\n",
      "  -0.00542581  0.00059676]\n",
      " [-0.03191575 -0.02857597 -0.01868967 -0.01617435 -0.02658081  0.01693271\n",
      "  -0.00324436  0.02405324 -0.03087668 -0.03921938  0.0367818   0.03327987\n",
      "  -0.04473532 -0.03911715  0.04092145  0.00955772 -0.00455239  0.02874171\n",
      "   0.00863316 -0.01248695]\n",
      " [-0.00726358 -0.02659889 -0.01468716  0.01620622 -0.02090106 -0.03192788\n",
      "  -0.02999078  0.00549091  0.0004215  -0.00162289 -0.04431893 -0.03534098\n",
      "   0.02901215  0.03029061  0.02456994  0.03457529 -0.04686897  0.03122219\n",
      "   0.00918368  0.01589011]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]\n",
      " [ 0.01747442  0.04896531 -0.01101112 -0.03716022 -0.03507421 -0.01057287\n",
      "   0.00062475 -0.01127305 -0.04680735 -0.01342485  0.0330816   0.0266037\n",
      "  -0.03138311  0.04880703  0.03540217  0.02546382 -0.04878474 -0.00182844\n",
      "   0.01848814  0.01999776]], shape=(20, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_LEN = max(tf_keras_tokenizer.index_word ) +1\n",
    "print(VOCAB_LEN)\n",
    "\n",
    "embedding_layer = layers.Embedding(VOCAB_LEN, EMBED_SIZE)\n",
    "\n",
    "result = embedding_layer(tf_keras_encoded)\n",
    "#result.numpy()\n",
    "\n",
    "#Voila, now I have a list of tokenized, vocabbed, and embedded words. \n",
    "#Happiness arises\n",
    "\n",
    "print(result.shape)\n",
    "print(type(result))\n",
    "print(result[5000]) #should be a EMBED_SIZE x EMBED_SIZE expression of 'revolut'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on).\n",
    "\n",
    "If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes (32, 10) (batch of 32 sequences of length 10) or (64, 15) (batch of 64 sequences of length 15).\n",
    "\n",
    "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a (2, 3) input batch and the output is (2, 3, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17796, 20, 20)\n",
      "(17796, 20, 20, 1)\n",
      "(17796,)\n",
      "(17796, 1)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]], shape=(17796, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#Split the generates tf_keras_encoded dataset (result) into training and testing\n",
    "#DATASET_SIZE = result.shape[0]\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((result, y_data))\n",
    "dataset.shuffle(buffer_size=1024)\n",
    "#.batch(32)\n",
    "\n",
    "result2 = tf.reshape(result, (-1, EMBED_SIZE, EMBED_SIZE,1))\n",
    "y_data2 = tf.reshape(y_data, (-1, 1))\n",
    "\n",
    "print(result.shape)\n",
    "print(result2.shape)\n",
    "print(y_data.shape)\n",
    "print(y_data2.shape)\n",
    "\n",
    "print(type(result))\n",
    "print(type(result2))\n",
    "\n",
    "print(y_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17796\n",
      "-4449\n",
      "(13347, 20, 20, 1)\n",
      "(13347, 1)\n",
      "(4449, 20, 20, 1)\n",
      "(4449, 1)\n"
     ]
    }
   ],
   "source": [
    "#hardcode the validation sample at 3000\n",
    "#not good practice, but hey, here we are\n",
    "\n",
    "print(result2.shape[0])\n",
    "cutoff=int(result2.shape[0]*-0.25)\n",
    "print(str(cutoff))\n",
    "\n",
    "x_val = result2[cutoff:]\n",
    "y_val = y_data2[cutoff:]\n",
    "x_train = result2[:cutoff]\n",
    "y_train = y_data2[:cutoff]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#256 h-length convolutional filters 3,4,5,6 - for WORD level\n",
    "wordmodel = models.Sequential()\n",
    "wordmodel.add(layers.Conv2D(filters=256,kernel_size=(3, 3), strides= (1, 1), padding='same', activation='relu', input_shape=(EMBED_SIZE, EMBED_SIZE, 1)))\n",
    "wordmodel.add(layers.MaxPooling2D((2, 2)))\n",
    "wordmodel.add(layers.Dense(512, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmodel.add(layers.Flatten())\n",
    "wordmodel.add(layers.Dense(512, activation='relu'))\n",
    "wordmodel.add(layers.Dense(256, activation='relu'))\n",
    "wordmodel.add(layers.Dense(128, activation='relu'))\n",
    "wordmodel.add(layers.Dense(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmodel.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),metrics=['accuracy'])\n",
    "\n",
    "#wordmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 256)       2560      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10, 10, 512)       131584    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               26214912  \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 26,513,667\n",
      "Trainable params: 26,513,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wordmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "376/376 [==============================] - 122s 325ms/step - loss: 1.6572 - accuracy: 0.8972 - val_loss: 1.6541 - val_accuracy: 0.8974\n",
      "Epoch 2/10\n",
      "376/376 [==============================] - 121s 322ms/step - loss: 1.6572 - accuracy: 0.8972 - val_loss: 1.6541 - val_accuracy: 0.8974\n",
      "Epoch 3/10\n",
      "376/376 [==============================] - 116s 308ms/step - loss: 1.6572 - accuracy: 0.8972 - val_loss: 1.6541 - val_accuracy: 0.8974\n",
      "Epoch 4/10\n",
      "153/376 [===========>..................] - ETA: 1:07 - loss: 1.7053 - accuracy: 0.8942"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-dafe0beeaf72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Why you need the epoch anyways...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Reminder change the epoch if needed it was 10\n",
    "#Why you need the epoch anyways...\n",
    "\n",
    "history = wordmodel.fit(x_train, y_train, epochs=10, validation_split=0.1, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "35/35 [==============================] - 7s 202ms/step - loss: 1.6701 - accuracy: 0.8964\n",
      "test loss, test acc: [1.670137643814087, 0.8963811993598938]\n",
      "Generate predictions for 3 samples\n",
      "predictions shape: (3, 3)\n",
      "[[ 2.6192245  -0.11465917 -0.16166943]\n",
      " [ 2.6192782  -0.11466367 -0.16098382]\n",
      " [ 2.6187303  -0.1142107  -0.1623353 ]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = wordmodel.evaluate(x_val,y_val, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = wordmodel.predict(x_val[:3])\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling and fitting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordmodel.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557/557 - 36s - loss: 1.6602 - accuracy: 0.8970\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['loss'], label = 'loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = wordmodel.evaluate(result2,  y_data2, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8969992995262146\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
